
# ğŸ©º Medical RAG Chatbot

A Retrieval-Augmented Generation (RAG) based AI chatbot that answers medical questions using uploaded PDF documents.

This system retrieves relevant content from medical PDFs and generates context-aware answers using a lightweight Hugging Face language model.

---

## ğŸš€ Features

- ğŸ“„ Multi-PDF support
- ğŸ” Semantic search using FAISS vector database
- ğŸ§  Context-based response generation
- âš¡ Lightweight LLM (flan-t5-small)
- ğŸŒ Streamlit interactive UI
- ğŸ“š Embeddings using sentence-transformers

---

## ğŸ—ï¸ Architecture

1. PDFs are loaded and processed.
2. Text is split into smaller chunks.
3. Chunks are converted into embeddings.
4. FAISS stores embeddings for similarity search.
5. User question is embedded and matched with relevant chunks.
6. Retrieved context is passed to the LLM.
7. Final answer is generated based on retrieved context.

---

## ğŸ› ï¸ Tech Stack

- Python
- LangChain
- FAISS
- Hugging Face Transformers
- Sentence-Transformers
- Streamlit

---

## ğŸ“‚ Project Structure

```

medical_chatbot/
â”‚
â”œâ”€â”€ data/                  # Medical PDFs
â”œâ”€â”€ vectorstore/           # FAISS index files
â”œâ”€â”€ create_memory_for_llm.py  # Builds vector database
â”œâ”€â”€ app.py                 # Streamlit application
â”œâ”€â”€ requirements.txt

```

---

Create virtual environment:

```

python -m venv .venv

```

Activate environment:

Windows:
```

.venv\Scripts\activate

```

Install dependencies:

```

pip install -r requirements.txt

```

---

## ğŸ§  Create Vector Database (Static Version)

Place your PDFs inside the `data/` folder and run:

```

python create_memory_for_llm.py

```

This will generate the FAISS vector database.

---

## â–¶ï¸ Run the Application

```

streamlit run app.py

```

---

## ğŸŒ Deployment

To deploy on Streamlit Community Cloud:

1. Push this project to a GitHub repository.
2. Connect the repository to Streamlit Cloud.
3. Select `app.py` as the main file.
4. Deploy.

---


